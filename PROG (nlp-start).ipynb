{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0748a43e",
   "metadata": {},
   "source": [
    "# Natural Language Processing. Основные понятия\n",
    "\n",
    "*Паршина Анастасия, НИУ ВШЭ*\n",
    "\n",
    "Сегодня мы с вами познакомимся с основными понятиями обработки естественного языка и разберем применение самых простых инструментов на небольших примерах. В дальнейшем все это нам сильно пригодится. Мы поговорим про понятия стемминга и лемматизации, также посмотрим на формирование n-грамм, узнаем, что такое стоп-слова, и посмотрим, как эти инструменты работают с русскоязычными текстами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4a598",
   "metadata": {},
   "source": [
    "## Стемминг и лемматизация\n",
    "\n",
    "### Стемминг\n",
    "\n",
    "Для стемминга нам понадобится импортировать модуль `nltk`, в котором есть реализация стеммера Портера. \n",
    "\n",
    "*Важно! Вероятро, при первом запуске команды `import nltk` у вас возникнет ошибка `ModuleNotFoundError: No module named 'nltk'`. Один из вариантов ее решения: запустить команду `!pip install nltk`, она написана для вас в виде комментария после знака `#`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c25a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b38d33",
   "metadata": {},
   "source": [
    "Далее нам предстоит долгий путь. Обратимся к модулю `nltk`, затем к пакету `stem` из него и модулю `porter`. В конце этой истории нам нужен класс `PorterStemmer()`.\n",
    "\n",
    "Вы можете вызвать команду `help(nltk.stem.porter)`, чтобы посмотреть подробную документацию, в частности — описание самого алгоритма стемминга Портера.\n",
    "\n",
    "У самого класса есть метод `.stem()`. Его можно применить так: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148468b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'argu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.stem.porter.PorterStemmer().stem('argued')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74d450",
   "metadata": {},
   "source": [
    "Но эта конструкция очень длинная, поэтому часть `nltk.stem.porter.PorterStemmer()` мы можем записать в отдельную перемнную, например, назовем ее `stemmer`. Затем уже можно применять метод `.stem()` к этой переменной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b4daf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argu\n",
      "argu\n",
      "argu\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('argue'))\n",
    "print(stemmer.stem('argued'))\n",
    "print(stemmer.stem('arguing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98725b",
   "metadata": {},
   "source": [
    "Обратите внимание на пример из презентации. Вот его наглядная реализация в Python.\n",
    "\n",
    "Если мы обратим внимание на документацию, то увидим, что в методе `.stem()` проставлен параметр `to_lowercase=True`, то есть слова автоматически приводятся к нижнему регистру (строчными буквами). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c165fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argu\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('ArGue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf1278",
   "metadata": {},
   "source": [
    "Также важно, что метод работает только с одним словом, ему нельзя передать сразу несколько, например, в виде списка. Будет выдана ошибка `AttributeError: 'list' object has no attribute 'lower'`, то есть к списку нельзя применить метод `.lower()`, потому что это метод строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04be3e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(stemmer\u001b[38;5;241m.\u001b[39mstem([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArGue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margue\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/stem/porter.py:658\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, to_lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m    :param to_lowercase: if `to_lowercase=True` the word always lowercase\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m     stem \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m to_lowercase \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNLTK_EXTENSIONS \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool[stem]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(['ArGue', 'argue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ef320",
   "metadata": {},
   "source": [
    "Однако мы можем обратиться сразу к нескольким словам в цикле `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62315204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argu\n",
      "argu\n",
      "argu\n"
     ]
    }
   ],
   "source": [
    "words = ('argue', 'argued', 'arguing')\n",
    "\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630611c",
   "metadata": {},
   "source": [
    "Исходный кортеж `words` в таком случае не изменился. Если мы хотим не просто напечатать результат, а сохранить его куда-то, то можно для этого использовать список и метод списков `.append()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "494c00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['argu', 'argu', 'argu']\n"
     ]
    }
   ],
   "source": [
    "stem_words = [] # Пустой список\n",
    "\n",
    "for word in words:\n",
    "    stem_words.append(stemmer.stem(word)) # Добавляем основу слова в список\n",
    "    \n",
    "print(stem_words) \n",
    "# Если нам нужны только уникальные значения, то можно использовать множество и метод .add()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9f8ae",
   "metadata": {},
   "source": [
    "Быстрее будут работать списковые включения (list comprehensions), которые можно реализовать так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e471562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['argu', 'argu', 'argu']\n"
     ]
    }
   ],
   "source": [
    "stem_words2 = [stemmer.stem(word) for word in words]\n",
    "print(stem_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a686c8f",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "Далее посмотрим, как работает лемматизация в Python.\n",
    "\n",
    "Аналогично стеммингу мы обращаемся к классу `WordNetLemmatizer()` и его методу `.lemmatize()`. Единственно отличие в том, что при запуске будет выдаваться ошибка `LookupError: Resource wordnet not found.`. Чтобы ее решить нужно запустить команду `nltk.download('wordnet')`.\n",
    "\n",
    "Можете ознакомиться с документацией, запустив команду `help(nltk.stem.wordnet)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c73cb9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argue\n",
      "argued\n",
      "arguing\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "lemm = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "print(lemm.lemmatize('argue'))\n",
    "print(lemm.lemmatize('argued'))\n",
    "print(lemm.lemmatize('arguing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bc336",
   "metadata": {},
   "source": [
    "Получили что-то странное, отличное от информации из презентации. На самом деле, у метода `.lemmatize()` есть параметр `pos`, то есть `Part of Speech` (часть речи), значение которого по умолчанию равно `n`, то есть noun (существительное).\n",
    "\n",
    "    \"n\" — nouns\n",
    "    \"v\" — verbs\n",
    "    \"a\" — adjectives\n",
    "    \"r\" — adverbs \n",
    "    \"s\" — satellite adjectives\n",
    "\n",
    "Дабы сделать лемматизацию более точной, мы можем указать, с какой частью речи мы работаем. \n",
    "\n",
    "Определение частей речи зашито в модуль `textblob`, из него нам нужен класс `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01408fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f388f1",
   "metadata": {},
   "source": [
    "Передадим ему наши слова в виде строки и вызовем атрибут `tags`, который выдаст нам список кортежей со словами и их частями речи.\n",
    "\n",
    "В нашем случае `NN` обозначает `Noun, singular or mass` (существительное), `VBD` — `Verb, past tense` (глагол прошедшего времени), `VBG` — `Verb, gerund or present participle` (глагол, герундий или причастие настоящего времени). Конечно, это все относится к английскому языку.\n",
    "\n",
    "Если ячейка выдает ошибку, то необходимо запустить первые две команды, убрав знак `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ba34f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('argue', 'NN'), ('argued', 'VBD'), ('arguing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "text_blob_object = TextBlob(('argue argued arguing'))\n",
    "print(text_blob_object.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34ccf5",
   "metadata": {},
   "source": [
    "Теперь попробуем провести лемматизацию для этих слов. \n",
    "\n",
    "Обратимся к каждому кортежу в нашем списке. Первый (нулевой по индексу) элемент кортежа — это наше слово. А второй (первый по индексу) — это часть речи. Однако, вы уже могли обратить внимание, что параметр `pos` принимает определенные значения (выписаны выше), значит, например, `NN` должно быть преобразовано в `n`, `VBD` — в `v` и т.д. На нашем простом примере достаточно забрать первый элемент строки, например, `N` из `NN` и привести к нижнему регистру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "738ef69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('argue', 'NN')\n",
      "argue\n",
      "('argued', 'VBD')\n",
      "argue\n",
      "('arguing', 'VBG')\n",
      "argue\n"
     ]
    }
   ],
   "source": [
    "words = text_blob_object.tags\n",
    "\n",
    "for word in words:\n",
    "    print(word)\n",
    "    print(lemm.lemmatize(word[0], pos=word[1][0].lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d7280",
   "metadata": {},
   "source": [
    "## N-граммы\n",
    "\n",
    "Это комбинации слов, стоящих рядом в предложениях. Метод, который позволяет получить N-граммы также реализован для класса `TextBlob` и называется он `.ngrams()`. На вход он принимает один параметр — целое число, обозначающее `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d4f5662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['argue']), WordList(['argued']), WordList(['arguing'])]\n",
      "[WordList(['argue', 'argued']), WordList(['argued', 'arguing'])]\n",
      "[WordList(['argue', 'argued', 'arguing'])]\n"
     ]
    }
   ],
   "source": [
    "# N = 1\n",
    "print(text_blob_object.ngrams(1))\n",
    "\n",
    "# N = 2\n",
    "print(text_blob_object.ngrams(2))\n",
    "\n",
    "# N = 3\n",
    "print(text_blob_object.ngrams(3))\n",
    "\n",
    "# и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04d290",
   "metadata": {},
   "source": [
    "Разберем пример из презентации со строкой `\"I love Python very much\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b6b2c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['I', 'love']), WordList(['love', 'Python']), WordList(['Python', 'very']), WordList(['very', 'much'])]\n"
     ]
    }
   ],
   "source": [
    "phrase_eng = \"I love Python very much\"\n",
    "text_blob_object = TextBlob(phrase_eng)\n",
    "\n",
    "# Вот, например, мы получили биграммы\n",
    "print(text_blob_object.ngrams(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9a660",
   "metadata": {},
   "source": [
    "Предположим, мы хотим получить вообще все возможные N-граммы фразы. Следовательно, максимальное значение `N` будет равно количеству слов в предлежении. \n",
    "\n",
    "Все количество слов можно узнать, обратившийсь к атрибуту `words`. Количество слов можно узнать, применив функцию `len()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24016eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(text_blob_object.words)) # максимальное N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbae14",
   "metadata": {},
   "source": [
    "Нам нужно сгенерировать значения N от 1 до 5 включительно. Для этого воспользуемся функцией `range()` с такими параметрами: `range(1, len(text_blob_object.words) + 1)`. Напомним, что значение `stop` функция `range()` не включает, поэтом дополнительно прибавляем один."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f5b77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1\n",
      "[WordList(['I']), WordList(['love']), WordList(['Python']), WordList(['very']), WordList(['much'])]\n",
      "N = 2\n",
      "[WordList(['I', 'love']), WordList(['love', 'Python']), WordList(['Python', 'very']), WordList(['very', 'much'])]\n",
      "N = 3\n",
      "[WordList(['I', 'love', 'Python']), WordList(['love', 'Python', 'very']), WordList(['Python', 'very', 'much'])]\n",
      "N = 4\n",
      "[WordList(['I', 'love', 'Python', 'very']), WordList(['love', 'Python', 'very', 'much'])]\n",
      "N = 5\n",
      "[WordList(['I', 'love', 'Python', 'very', 'much'])]\n"
     ]
    }
   ],
   "source": [
    "for N in range(1, len(text_blob_object.words) + 1):\n",
    "    print(f'N = {N}')\n",
    "    print(text_blob_object.ngrams(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29855f5",
   "metadata": {},
   "source": [
    "Можем также распаковать результат работы метода `.ngrams()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b9f08b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1\n",
      "I\n",
      "love\n",
      "Python\n",
      "very\n",
      "much\n",
      "N = 2\n",
      "I love\n",
      "love Python\n",
      "Python very\n",
      "very much\n",
      "N = 3\n",
      "I love Python\n",
      "love Python very\n",
      "Python very much\n",
      "N = 4\n",
      "I love Python very\n",
      "love Python very much\n",
      "N = 5\n",
      "I love Python very much\n"
     ]
    }
   ],
   "source": [
    "for N in range(1, len(text_blob_object.words) + 1):\n",
    "    print(f'N = {N}')\n",
    "    for ngram in text_blob_object.ngrams(N):\n",
    "        print(*ngram) # Знак * в функции print() отвечает за распаковку списка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257293a",
   "metadata": {},
   "source": [
    "## Стоп-слова и их удаление\n",
    "\n",
    "Стоп-слова — это своеобразный шум, который мешает при обработке даных, например, предлоги, союзы, междометия, поэтому их часто убирают. Давайте посмотрим на стоп-слова в английском языке. \n",
    "\n",
    "Нам все также понадобится модуль `nltk`. Предварительно запустите команду `nltk.download(\"stopwords\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9303444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download(\"stopwords\") \n",
    "stop = nltk.corpus.stopwords\n",
    "print(stop.words(\"english\")) # Все стоп-слова английского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6e2f4",
   "metadata": {},
   "source": [
    "Обратите внимание, что это список. \n",
    "\n",
    "Попробуем убрать стоп-слова из предложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47b19786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My God! Your eyes are as beautiful as the sky, your soul is reflected in them. It is pure and immaculate as the soul of a baby. My lovely beautiful soul! I am in love with you!\n"
     ]
    }
   ],
   "source": [
    "text = 'My God! Your eyes are as beautiful as the sky, your soul is reflected in them. \\\n",
    "It is pure and immaculate as the soul of a baby. My lovely beautiful soul! I am in love with you!'\n",
    "\n",
    "# Обратите внимание на символ \\ — он позволяет перенести код на седующую строку и не мешает его работе\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc8d88",
   "metadata": {},
   "source": [
    "Сначала соберем все слова из нашего текста с помощью класса `TextBlob` и атрибута `words`. Предватительно все слова в тексте приведем к нижнему регистру с помощью метода `.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39177616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'god', 'your', 'eyes', 'are', 'as', 'beautiful', 'as', 'the', 'sky', 'your', 'soul', 'is', 'reflected', 'in', 'them', 'it', 'is', 'pure', 'and', 'immaculate', 'as', 'the', 'soul', 'of', 'a', 'baby', 'my', 'lovely', 'beautiful', 'soul', 'i', 'am', 'in', 'love', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "text_blob_object = TextBlob(text.lower())\n",
    "print(text_blob_object.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f700a33",
   "metadata": {},
   "source": [
    "Далее вариантов работы несколько. Например, если нам не важен порядок слов, то мы можем использовать разность множеств, то есть \n",
    "\n",
    "    1. Все слова нашего такста сделать множество \n",
    "    2. Преобразовать список со стоп-словами во множество \n",
    "    3. Из первого множества вычесть второе (или использовать метод .difference())\n",
    "    \n",
    "Напомним, что в множествах элементы уникальные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18df829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky', 'beautiful', 'baby', 'lovely', 'immaculate', 'soul', 'god', 'reflected', 'eyes', 'pure', 'love'}\n",
      "{'sky', 'beautiful', 'baby', 'lovely', 'immaculate', 'soul', 'god', 'reflected', 'eyes', 'pure', 'love'}\n"
     ]
    }
   ],
   "source": [
    "print(set(text_blob_object.words) - set(stop.words(\"english\")))\n",
    "print(set(text_blob_object.words).difference(set(stop.words(\"english\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f56060",
   "metadata": {},
   "source": [
    "Если же нам важно сохранить порядок слов, то можно использовать цикл `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bcd6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['god', 'eyes', 'beautiful', 'sky', 'soul', 'reflected', 'pure', 'immaculate', 'soul', 'baby', 'lovely', 'beautiful', 'soul', 'love']\n"
     ]
    }
   ],
   "source": [
    "result = [] # Пустой список\n",
    "\n",
    "for word in text_blob_object.words:        # Обращаемся к каждому слову в тексте\n",
    "    if word not in stop.words(\"english\"):  # Если слова нет в стоп-словах, то \n",
    "        result.append(word)                # добавляем его в список\n",
    "        \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17adae7",
   "metadata": {},
   "source": [
    "Аналогично можно использовать списковые включения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e6b96ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['god', 'eyes', 'beautiful', 'sky', 'soul', 'reflected', 'pure', 'immaculate', 'soul', 'baby', 'lovely', 'beautiful', 'soul', 'love']\n"
     ]
    }
   ],
   "source": [
    "result2 = [word for word in text_blob_object.words if word not in stop.words(\"english\")]\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2bdd1b",
   "metadata": {},
   "source": [
    "## Работа с русским языком\n",
    "\n",
    "Теперь проверим, как все вышеперечисленное работает с русским языком. Начнем со стеммера Портера. \n",
    "\n",
    "### Стемминг\n",
    "\n",
    "Используем тот алгоритм, что и выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "63872b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежать\n",
      "бегущий\n",
      "бегающий\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('бежать'))\n",
    "print(stemmer.stem('бегущий'))\n",
    "print(stemmer.stem('бегающий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd243b",
   "metadata": {},
   "source": [
    "Почему-то ничего не изменилось. Это связано с тем, что оригинальный стеммер Портера работает только с английским языком. Однако, данный алгоритм был доработан и для других языков. \n",
    "\n",
    "Мы все еще работаем с модулем `nltk`, обращаемся к классу `RussianStemmer()`.\n",
    "\n",
    "Если посмотреть документацию с помощью команды `help(nltk.stem.snowball)`, то можно увидеть и другие языки, для которых реализован алгоритм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d02361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежа\n",
      "бегущ\n",
      "бега\n"
     ]
    }
   ],
   "source": [
    "rus_stemmer =  nltk.stem.snowball.RussianStemmer()\n",
    "\n",
    "print(rus_stemmer.stem('бежать'))\n",
    "print(rus_stemmer.stem('бегущий'))\n",
    "print(rus_stemmer.stem('бегающий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efb323",
   "metadata": {},
   "source": [
    "Однако, мы видим, что стемминг не всегда точно работает с русским языком, и лучше будет использовать лемматизацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661a44b",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "Аналогично посмотрим, как работает метод `.lemmatize()` с русскими словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0f488f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежать\n",
      "бегущий\n",
      "бегающий\n"
     ]
    }
   ],
   "source": [
    "print(lemm.lemmatize('бежать'))\n",
    "print(lemm.lemmatize('бегущий'))\n",
    "print(lemm.lemmatize('бегающий'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009c578",
   "metadata": {},
   "source": [
    "И снова не работает! Для работы и лемматизации русских слов комфортнее использовать модуль `pymorphy3`. При первом запуске вам необходимо будет запустить команду `!pip install pymorphy3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87eac355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymorphy3\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ed03d",
   "metadata": {},
   "source": [
    "Доберемся до класса `MorphAnalyzer()` и используем его метод `.parse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "18ee42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='бежать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='бежать', score=0.5, methods_stack=((DictionaryAnalyzer(), 'бежать', 392, 0),))\n",
      "Parse(word='бежать', tag=OpencorporaTag('INFN,impf,intr'), normal_form='бежать', score=0.5, methods_stack=((DictionaryAnalyzer(), 'бежать', 392, 42),))\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy3.analyzer.MorphAnalyzer()\n",
    "\n",
    "for data in morph.parse('бежать'):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f2164",
   "metadata": {},
   "source": [
    "Давайте разберемся, почему было выдано два варианта и что они вообще означают.\n",
    "\n",
    "Нам нужно обратить внимание на `tag` в обоих случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7e56b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFN,perf,intr\n",
      "INFN,impf,intr\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('бежать')[0].tag)\n",
    "print(morph.parse('бежать')[1].tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "69081285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intr'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('бегущий')[0]\n",
    "p.tag.transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f5094",
   "metadata": {},
   "source": [
    "Разберемся, что означает каждый из них:\n",
    "    \n",
    "    INFN,perf,intr — глагол инфинитив, совершенного вида, непереходный\n",
    "    INFN,impf,intr — глагол инфинитив, несовершенного вида, непереходный\n",
    "    \n",
    "*Небольшие пояснения:*\n",
    "\n",
    "+ Совершенный вид глагода отвечает на вопрос \"что сделать?\". Например, \"Они бросились врассыпную, пытаясь бежать от служителей порядка\". \n",
    "\n",
    "+ Несовершенный вид глагола отвечает на вопрос \"что делать?\". Например, \"Чтобы спастись, нужно бежать далеко и быстро\".\n",
    "\n",
    "+ Непереходные глаголы обозначают действия, не направленные на какой-либо предмет. Например, в нашем случае \"Чтобы спастись, нужно бежать далеко и быстро\".\n",
    "\n",
    "+ Переходные глаголы — глаголы, действие которых направлено на другой предмет. Например, \"Нужно хорошо подготовиться, чтобы бежать дистанцию\". Обратите внимание, что даже `MorphAnalyzer()` не все знает про русские слова, однако, многое он определяет достаточно точно.\n",
    "    \n",
    "С обозначениями можно ознакомиться на официальном сайте с [документацией](https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs).\n",
    "\n",
    "Также нам может быть интересно значение атрибута `score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7c231600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('бежать')[0].score)\n",
    "print(morph.parse('бежать')[1].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9cbcc",
   "metadata": {},
   "source": [
    "В нашем случае оба равны `0.5`, то есть в равной степени может существовать как первая интерпретация глагола, так и вторая.\n",
    "\n",
    "Посмотрим на слово `\"были\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8b5f8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='были', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='быть', score=0.997032, methods_stack=((DictionaryAnalyzer(), 'были', 620, 7),))\n",
      "Parse(word='были', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='быль', score=0.000593, methods_stack=((DictionaryAnalyzer(), 'были', 13, 1),))\n",
      "Parse(word='были', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='быль', score=0.000593, methods_stack=((DictionaryAnalyzer(), 'были', 13, 2),))\n",
      "Parse(word='были', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='быль', score=0.000593, methods_stack=((DictionaryAnalyzer(), 'были', 13, 5),))\n",
      "Parse(word='были', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='быль', score=0.000593, methods_stack=((DictionaryAnalyzer(), 'были', 13, 6),))\n",
      "Parse(word='были', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='быль', score=0.000593, methods_stack=((DictionaryAnalyzer(), 'были', 13, 9),))\n"
     ]
    }
   ],
   "source": [
    "for data in morph.parse('были'):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599e6bc",
   "metadata": {},
   "source": [
    "Здесь вариантов уже больше. Наиболее вероятный — первый, со значением `score=0.997032`, то есть с большой вероятностью мы имеем дело с глаголом прошедшего времени, нормальная форма которого `быть`. Однако это может быть и множественное число слова `быль`, хотя и с меньшей вероятностью.\n",
    "\n",
    "Суммарно значение `score` должно быть равно 1, однако из-за особенностей работы с вещественными числами, оно всегда будет около `0.999999`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bf6a4",
   "metadata": {},
   "source": [
    "Чтобы забрать нормальную форму слова, нужно обратиться к ней с помощью атрибута `normal_form`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7b55bec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'быть'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('были')[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876db0",
   "metadata": {},
   "source": [
    "### N-граммы\n",
    "\n",
    "А вот работа с N-граммами не меняется, все остается также, как и было."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9f67fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['Я', 'очень']), WordList(['очень', 'сильно']), WordList(['сильно', 'люблю']), WordList(['люблю', 'Python'])]\n"
     ]
    }
   ],
   "source": [
    "phrase_rus = \"Я очень сильно люблю Python\"\n",
    "text_blob_object = TextBlob(phrase_rus)\n",
    "\n",
    "# Вот, например, мы получили биграммы\n",
    "print(text_blob_object.ngrams(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986301a8",
   "metadata": {},
   "source": [
    "Аналогично можно посмотреть вообще все N-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bf9b97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1\n",
      "Я\n",
      "очень\n",
      "сильно\n",
      "люблю\n",
      "Python\n",
      "N = 2\n",
      "Я очень\n",
      "очень сильно\n",
      "сильно люблю\n",
      "люблю Python\n",
      "N = 3\n",
      "Я очень сильно\n",
      "очень сильно люблю\n",
      "сильно люблю Python\n",
      "N = 4\n",
      "Я очень сильно люблю\n",
      "очень сильно люблю Python\n",
      "N = 5\n",
      "Я очень сильно люблю Python\n"
     ]
    }
   ],
   "source": [
    "for N in range(1, len(text_blob_object.words) + 1):\n",
    "    print(f'N = {N}')\n",
    "    for ngram in text_blob_object.ngrams(N):\n",
    "        print(*ngram) # Знак * в функции print() отвечает за распаковку списка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36a4fa",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов\n",
    "\n",
    "Очевидно, что в русском языке свой набор стоп-слов. Вызвать его можно также с помощью модуля `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "346524a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stop.words(\"russian\")) # Все стоп-слова русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc90c70",
   "metadata": {},
   "source": [
    "Логика удаления стоп-слов не меняется. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4280ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Боже мой! Твои глаза прекрасны, как небо, в них отражается твоя душа. Она чиста и непорочна, как душа ребенка. Моя милая, прекрасная душа! Я влюблен в тебя!\n"
     ]
    }
   ],
   "source": [
    "text_rus = 'Боже мой! Твои глаза прекрасны, как небо, в них отражается твоя душа. \\\n",
    "Она чиста и непорочна, как душа ребенка. Моя милая, прекрасная душа! Я влюблен в тебя!'\n",
    "\n",
    "# Обратите внимание на символ \\ — он позволяет перенести код на седующую строку и не мешает его работе\n",
    "\n",
    "print(text_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3a761522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['боже', 'мой', 'твои', 'глаза', 'прекрасны', 'как', 'небо', 'в', 'них', 'отражается', 'твоя', 'душа', 'она', 'чиста', 'и', 'непорочна', 'как', 'душа', 'ребенка', 'моя', 'милая', 'прекрасная', 'душа', 'я', 'влюблен', 'в', 'тебя']\n"
     ]
    }
   ],
   "source": [
    "text_blob_object = TextBlob(text_rus.lower())\n",
    "print(text_blob_object.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c5699ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['боже', 'твои', 'глаза', 'прекрасны', 'небо', 'отражается', 'твоя', 'душа', 'чиста', 'непорочна', 'душа', 'ребенка', 'милая', 'прекрасная', 'душа', 'влюблен']\n"
     ]
    }
   ],
   "source": [
    "result_rus = [word for word in text_blob_object.words if word not in stop.words(\"russian\")]\n",
    "print(result_rus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
